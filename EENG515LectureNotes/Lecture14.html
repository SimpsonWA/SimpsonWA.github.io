
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Markdown with MathJax</title>
    
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js">
</script>

</head>
<body>
    <h2><strong>Left and Right Inverses</strong>:</h2>

<ul>
<li><strong>Left Inverse</strong>: A matrix A has a left inverse if \(\exists B  \ni BA = I\) (there exists a B such that BA = I)</li>
<li><strong>Right Inverse</strong>: A matrix A has a right inverse if \(\exists C \ni AC = I\) (there exists a C such that AC = I)</li>
<li><p><strong>Inverse</strong>: A square matrix A has an inverse if \(\exists A^{-1} \ni A^{-1}A = AA^{-1} = I\)</p></li>
<li><p>Properties:</p>

<ul>
<li>If \(A\) has a <code>left inverse</code> \(B\) then \(A\vec{x} = \vec{y}\) has a solution \(\vec{x} = B \vec{y}\)</li>
<li>If \(A\) has a <code>right inverse</code> \(C\) then \(A \vec{x} =\vec{y}\) has a solution \(\vec{x}=C\vec{y}\)</li>
</ul></li>
<li><p>Suppose \(A \in \mathbb{R}^{m \times n}\) and consider \(A\vec{x}=\vec{y}\):</p>

<ul>
<li><strong>Existence</strong>: if \(rank(A)=r=m\leq n\) (columns of \(A\) span \(\mathbb{R}^m\)), then:
<ul>
<li>There exists a right inverse \(C\), typically of form \(C = A^{T}(AA^T)^{-1}\)</li>
<li>For any \(\vec{x}\in \mathbb{R}^m\), there exists at least on solution \(\vec{x} = C\vec{y}\)</li>
<li>A solution exists only if \(\vec{y} \in \cal{R}(A)\), which implies \(\vec{y} \perp \cal{N}(A^*)\) : these points are called the <strong>Fredholm alternative Theorem</strong></li>
</ul></li>
<li><strong>Uniqueness</strong>: if \(rank(A)=r=n\leq m\) (columns of \(A\) are linearly independent), then:
<ul>
<li>There exists a left inverse \(B\), typically of the form \(B = (A^TA)^{-1}A^T\)</li>
<li>For any \(\vec{y} \in \mathbb{R}^m\), there exists at most one solution \(\vec{x} = B \vec{y}\) </li>
<li>If \(\vec{y}\in \cal{R}(A)\) and \(\cal{N}(A^*) \neq {0}\) then there are infinite number of solutions to \(A\vec{x}=\vec{y}\)</li>
</ul></li>
<li><strong>Invertibility</strong>: (Existence and Uniqueness): if \(rank(A)=r=n=m\) (\(A\) is square and full rank) then:
<ul>
<li>There exists an inverse \(A^{-1} = B = C\)</li>
<li>For any \(\vec{y} \in \mathbb{R}^m\) there exists one and only one solution \(\vec{x} = A^{-1}\vec{y}\)</li>
</ul></li>
</ul></li>
</ul>

<h2>Pseudoinverse Operators</h2>

<ul>
<li><strong>Pseudoinverse</strong>: let \(A:X\rightarrow Y\) be a bounded linear operator between 2 inner product spaces \(X\) and \(Y\).
<ul>
<li>1.) Given any \(\vec{y} \in Y\) let \(\vec{x} \in X\) be the vector that solves:
<ul>
<li>\(min_{\vec{x}\in X} ||\vec{y}-A\vec{x}||_Y\)</li>
</ul></li>
<li>2.) If there are multiple minimizers, let \(\vec{x}'\) denote the one having minimal norm \(||\vec{x}'||_X\)</li>
</ul></li>
<li>The pseudoinverse of \(A\) denoted \(A^+: Y \rightarrow X\) is defined as the operator mapping any \(\vec{y} \in Y\) to the corresponding \(\vec{x}' \in X\)</li>
<li><strong>Facts of Pseudoinverses</strong>:
<ul>
<li>The pseudoinverse is always a bounded linear operator</li>
<li>Special Cases
<ul>
<li>If \(A\) is invertible then \(A^+ = A^{-1}\)</li>
<li>If \(A^_A\) is invertible, then \(A^+ = (A^_A)^{-1}A^_\) (left inverse)</li>
<li>If \(AA^_\) is invertible then \(A^+ = A^_(AA^_)^{-1}\)</li>
</ul></li>
<li>Other Properties:
<ul>
<li>\((A^+)^+ = A\)</li>
<li>\((A^+)^* = (A^*)^+\)</li>
<li>\(A^+AA^+ = A^+\)</li>
<li>\(AA^+A = A\)</li>
<li>\(A^+A\) and \(AA^+\) are self Adjoint</li>
<li>\(\cal(R)(A^+) = \cal(R)(A^_)\) and \(\cal(N)(A^+) = \cal(N)(A^_)\) </li>
</ul></li>
</ul></li>
<li>Pseudoinverses have a useful connection to orthogonal projections specifically
<ul>
<li>\(AA^+\) is orthogonal projection operator onto \(\cal{R}(A)\). </li>
<li>\(A^+A\) is an orthogonal projection operator onto \(\cal{R}(A^*)\)</li>
</ul></li>
</ul>

<h2>Matrix Case</h2>

<ul>
<li>Consider \(X\in \mathbb{R}^n\) and \(Y \in \mathbb{R}^m\) and \(A:X \rightarrow Y\):
<ul>
<li>Given any \(\vec{y} \in Y\) let \(\vec{x}\in X\) be a vector that solves:
<ul>
<li>\(min_{\vec{x}\in X} ||\vec{y}-A\vec{x}||_2\)</li>
</ul></li>
<li>and if there are multiple minimizers, let \(\vec{x}'\) be the only one having minimal norm \(||\vec{x}'||_2\)</li>
<li>The pseudoinverse of \(A\) denoted \(A^+\) is a \(n \times m\) matrix that allows us to compute for any \(\vec{y}\) the corresponding \(\vec{x}'\)
<ul>
<li>\(\vec{x}' = A^+ \vec{y}\)</li>
</ul></li>
</ul></li>
<li>Useful facts and expressions about matrix pseudoinverse:
<ul>
<li>If \(A\) is invertible (this requires \(m = n\) and \(rank(A)=m=n\)) then
<ul>
<li>\(A^+ = A^{-1}\)</li>
</ul></li>
<li>If \(A\) has linearly independent columns (this requires \(m \geq n\) and \(rank(A) = n\)) then (left inverse):
<ul>
<li>\(A^+ = (A^HA)^{-1}A^H\)</li>
</ul></li>
<li>If \(A\) has linearly independent rows (\(m \leq n\) and \(rank(A)=m\)) then (right inverse):
<ul>
<li>\(A^+ = A^H(AA^H)^{-1}\)</li>
</ul></li>
</ul></li>
<li>Other properties for matrix case: ![[Pasted image 20240911083835.png]]</li>
</ul>

<h2>Normal Equations:</h2>

<ul>
<li>Repeated notes from Normla Eq</li>
<li>Now lets see how this can all be interpreted using operators, Adjoint, and pseudoinverses:
<ul>
<li>Define linear operator : \(V: R^n \rightarrow S\) by eq:
<ul>
<li>\(V\vec{a} = \sum_{k=1}^{n} a_k \vec{v_k}\)</li>
</ul></li>
<li>Notice the best approximation is by finding \(\vec{a}\) that minimizes \(||\vec{x}-V\vec{a}||_S\)</li>
<li>Know that the minimizer \(\vec{a}\) must satisfy:
<ul>
<li>\(V^V\vec{a} = V^\vec{x}\)</li>
</ul></li>
<li>by def the Adjoint of \(V(V^*)\) denoted as \(V^*: S \rightarrow R^n\) is a unique operator such that for all \(\vec{z} \in R^n\) and \(\vec{y} \in S\):
<ul>
<li>\( < V \vec{z},\vec{y}\gt >_S = \lt;\vec{z},V^_\vec{y}\gt;_l2\)</li>
</ul></li>
<li>We can write:
<ul>
<li>\(<V\vec{z},\vec{y}&gt;_s = &lt;\sum_{k=1}^{n}z_k\vec{v_k}, \vec{y}&gt; = sum_{k=1}^{n} z_k &lt;\vec{v_k},\vec{y}&gt;_S\)</li>
</ul></li>
<li>and
<ul>
<li>\(&lt;\vec{z},V^_ \vec{y}&gt;_l2 = \sum_{}^{} z_k((V^* \vec{y})_k)^_\)</li>
</ul></li>
<li>so
<ul>
<li>\(V^_\vec{y} = \begin{bmatrix} &lt;\vec{y},\vec{v_1}&gt;_s\ &lt;\vec{y},\vec{v_2}&gt;_s\ ....\ &lt;\vec{y},\vec{v_n}&gt;_s \end{bmatrix}\)</li>
<li>s</li>
</ul></li>
</ul></li>
</ul>

</body>
</html>
