
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Markdown with MathJax</title>
    
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js">
</script>

</head>
<body>
    <ul>
<li><p><strong>Theorem (Eigenvectors corresponding to distinct eigenvalues)</strong>: Eigenvectors corresponding to distinct eigenvalues are linearly independent</p>

<h2>Diagonalization:</h2></li>
<li><p>If \(A\) is an \((n \times n)\) matrix that happens to have \(n\) linearly independent eigenvectors, then it can be written (or diagonalized) as:</p></li>
</ul>

<p>$$
A=T \Lambda T^{-1} \quad (1)
$$
- \((1)\) Where \(T\) is an \(n \times n\) invertible matrix, containing the eigenvectors of \(A\)
- \((1)\) \(\Lambda\) is a \(n \times n\) diagonal matrix containing the eigenvalues of \(A\)</p>

<h2>Understanding and applying the operator A:</h2>

<ul>
<li>Let \(\vec{x} \in \mathbb{C}^n\) then:
$$
A\vec{x} = T \Lambda T^{-1} \vec{x} \quad (2)
$$\</li>
<li><strong>1st</strong>: \((2)\) Let \(\vec{x} = \alpha_1 \vec{v_1} + \alpha_2 \vec{v_2} + ... \alpha_n \vec{v_n}\) for some coefficients \(\alpha_1 , \alpha_2,...,\alpha_n\) </li>
<li><strong>1st</strong>:\((2)\) Then we can write \(\vec{x} = T \vec{\alpha}\) where:
$$
\vec{\alpha} = \begin{bmatrix} \alpha_1 \\ ...\\ \alpha_n \end{bmatrix} \in \mathbb{C}^n \quad (3)
$$</li>
<li><strong>1st</strong>: \((3)\) since \(T\) is invertible \(\vec{\alpha}\) must satisfy:
$$
\vec{\alpha} = T^{-1}\vec{x} \quad (4)
$$</li>
<li><strong>1st</strong>: \((4)\) Thus \(T^{-1}\vec{x}\) computes the expansion Coefficients needed to express \(\vec{x}\) in terms of eigenvectors of \(A\)</li>
<li><strong>2nd</strong>: Note that in the expression \((2)\) the portion:
$$
\Lambda T^{-1} \vec{x} = \Lambda \vec{\alpha} = \begin{bmatrix} \lambda_1 \alpha_1 \\ ...\\ \lambda_n \alpha_n \end{bmatrix} \quad (5)
$$</li>
<li><strong>3rd</strong> Thus:
$$
A\vec{x} = T\Lambda T^{-1}\vec{x} = 
\begin{bmatrix}
\vec{v_1} \ \vec{v_2} \ ... \ \vec{v_n} 
\end{bmatrix}
\begin{bmatrix}
\lambda_1 \alpha_1 \\
...\\
\lambda_n \alpha_n 
\end{bmatrix} \quad (6)
$$</li>
<li><p><strong>3rd</strong>: Rebuilds a signal as a linear combination of eigenvectors, but now each expansion has been scaled by the corresponding eigenvalue of \(A\) </p></li>
<li><p>Thus diagonalization can be summarized in 3 steps:</p></li>
</ul>

<ol>
<li>Transform (Find the \(\alpha_i\)'s)</li>
<li>Multiply (Scale the \(\alpha_i\)'s by the \(\lambda_i\)'s)</li>
<li>Inverse Transform (Rebuild using \(\vec{v_i}\)'s)</li>
</ol>

<h2>Diagonalizations of Self-Adjoint Matrices</h2>

<ul>
<li><strong>Lemma</strong> (Eigenvalues of self Adjoint matrix): If \(A = A^{H}\) then all eigenvalues of \(A\) are real valued (even if \(A\) itself has complex entries)</li>
<li><strong>Lemma</strong> (Eigenvectors of self Adjoint matrix): If \(A = A^{H}\) then there exists a set of \(n\) orthonormal eigenvectors \(\vec{v_1} \ \vec{v_2} \ ... \ \vec{v_n}\) such that:
$$
A\vec{v_i} = \lambda_i \vec{v_i} \quad (7)
$$</li>
<li>\((7)\): for all \(i = 1,2,...,n\)</li>
</ul>

<h2>Positive Definite Matrices: Definition</h2>

<ul>
<li>Let \(A\) be a square \(n \times n\) Hermitian matrix such that \(A = A^H\)</li>
<li>Recall that we say \(A\) is positive definite if:
$$
\vec{x}^H A \vec{x} &gt; 0 \quad (8)
$$</li>
<li>\((8)\) holds for all non-zero \(\vec{x} \in \mathbb{R}^n\) or \(\mathbb{C}^n\)</li>
<li>Similarly we say that \(A\) is <strong>positive semi-definite</strong> if 
$$
\vec{x}^H A \vec{x} \geq 0 \quad (9)
$$</li>
<li>\((9)\) hold for all non-zero \(\vec{x} \in \mathbb{R}^n\) or \(\mathbb{C}^n\)</li>
</ul>

<h2>Positive Definite Matrices: Eigenvalues:</h2>

<ul>
<li>If \(A = A^H\) we already know the eigenvalues of \(A\) are real</li>
<li>If \(A\) is positive definite then all eigenvalues of \(A\) are positive</li>
<li>If \(A\) is positive semi-definite then all eigenvalues of \(A\) are non-negative</li>
</ul>

</body>
</html>
