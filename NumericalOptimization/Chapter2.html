
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Markdown with MathJax</title>
    
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js">
</script>

</head>
<body>
    <p><strong>Unconstrained Optimization</strong>: Minimize objective function with no restrictions on the value of variables formulated as:
$$
min_{x}f(x)
$$
- \(x \in \mathbb{R}^n\): real vector with \(n \geq 1\) components
- \(f: \mathbb{R}^{n}\rightarrow \mathbb{R}\)</p>

<h2>2.1 What is the solution</h2>

<p><strong>Global minimizer</strong> of \(f\):
- a point \(x^{*}\) is a global minimizer if \(f(x^{*}) \leq f(x)\) for all x
<strong>Local Minimizer</strong> (weak) of \(f\):
- A point \(x^{*}\) is a local minimizer if there is a neighborhood \(\mathcal{N}\) of \(x^{*}\) such that \(f(x^{*}) \leq f(x)\) for all \(x \in \mathcal{N}\) 
- \(\mathcal{N}\) or neighborhood of \(x^{*}\) is just an open set that contains \(x^{*}\)
(Strict) <strong>Local Minimizer</strong> of \(f\):
- A point \(x^{*}\) is a strict local minimizer if there is a neighborhood \(\mathcal{N}\) of \(x^{*}\) such that \(f(x^{*}) &lt; f(x)\) for all \(x \in \mathcal{N}\) with \(x \neq x^{*}\)
(Isolated) <strong>Local Minimizer</strong> of \(f\):
- A point \(x^{*}\) is an isolated local minimizer if there is a neighborhood \(\mathcal{N}\) of \(x^{*}\) such that \(x^{*}\) is the only local minimizer in \(\mathcal{N}\)</p>

<h3>Recognizing a local minimum</h3>

<ul>
<li>When \(f\) is smooth (\(f\) is twice differentiable) we can tell that \(x^{*}\) is a local minimizer (and possibly strict) by examining the gradient \(\nabla f(x^{*})\) and the Hessian \(\nabla^{2}f(x^{*})\)</li>
</ul>

<p>Theorem 2.1: <strong>Taylor's Theorem</strong>:
- Suppose that \(f: \mathbb{R}^{n}\rightarrow \mathbb{R}\) is continuously differentiable and that \(p \in \mathbb{R}^n\) then we would have:
$$
f(x+p) = f(x) + \nabla f(x+tp)^{T}p 
$$
- for some \(t \in (0,1)\). Moreover if \(f\) is twice continuously differentiable we have that:
$$
\nabla f(x+p) = \nabla f(x) \int_{0}^{1} \nabla^{2}f(x+tp)p \ dt
$$
- and that:
$$
f(x+p) = f(x) + \nabla f(x)^{T}p + \frac{1}{2}p^{T}\nabla^{2}f(x+tp)p
$$
- for some \(t \in (0,1)\)</p>

<p>Theorem 2.2: <strong>First order necessary Conditions</strong>:
- If \(x^{*}\) is a local minimizer and \(f\) is continuously differentiable in an open neighborhood of \(x^{*}\) then \(\nabla f(x^{*}) =0\)</p>

<p>Theorem 2.3: <strong>Second Order Necessary Conditions</strong>:
- if \(x^{*}\) is a local minimizer of \(f\) and \(\nabla^{2}f\) exists and is continuous in an open neighborhood of \(x^{*}\) then \(\nabla f(x^{*}) =0\) and \(\nabla^{2}f(x^{*})\) is positive semidefinite</p>

<p>Theorem 2.4: <strong>Second order sufficient conditions</strong>
- Suppose that \(\nabla^{2}f\) is continuous in an open neighborhood of \(x^{*}\) and that \(\nabla f(x^{*}) =0\) and  \(\nabla^{2}f(x^{*})\) is positive semidefinite. Then \(x^{*}\) is a strict local minimizer of \(f\)</p>

<p>Theorem 2.5:
- When \(f\) is convex any local minimizer \(x^{*}\) is a global minimizer of \(f\). Also if \(f\) is differentiable, then any stationary point \(x^{*}\) is a global minimizer of \(f\)</p>

<h2>2.2 Overview of Algorithms</h2>

<ul>
<li>Commonly user selects starting point \(x_{0}\)'</li>
<li>Beginning at \(x_{0}\) the algorithm generates a sequence of iterations \({x_k}_{k=0}^{\infty}\) that terminates either when no progress is made or sufficiently reached a solution</li>
</ul>

<h3>2 Strategies: Line search and Trust region</h3>

<p><strong>Line Search</strong>:
- Algorithm chooses a direction \(p_{k}\) and searches along this direction from the current iteration \(x_{k}\) for a new iteration with a lower value 
- Distance along \(p_{k}\) can be found by solving following minimization problem to find a step length \(\alpha\):
$$
min_{\alpha &gt; 0 } f(x_{k}+ \alpha p_{k})
$$
- The line search algorithm generates a limited number of trail steps lengths until it finds one that loosely approximates the equation above. At the new point, a new search direction and step length are committed and process iterates. </p>

<p><strong>Trust Region</strong>:
- Info about \(f\) is gather to construct \(m_{k}\) (model function) whose behavior near \(x_{k}\) is similar to that of \(f\)
- We restrict the search for \(m_k\) to some region near \(x_{k}\) in other words candidate step \(p\) by approximating:
$$
min_{p}m_{k}(x_{k}+p)
$$
- Where \(x_{k}+p\) lies inside the trust region
- If candidate solution does not provide a sufficient increase in \(f\) we can conclude the trust region is too large</p>

</body>
</html>
