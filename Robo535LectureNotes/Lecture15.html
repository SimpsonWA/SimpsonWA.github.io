
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Markdown with MathJax</title>
    
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js">
</script>

</head>
<body>
    <h2>Linear Model for Regression</h2>

<ul>
<li>Replace inputs to linear units with feature (<strong>basis</strong>) functions to model nonlinearities:
$$
f(x) = w_{0} + \sum\limits_{j=1}^{m} w_{j}\phi_{j}(x)
$$</li>
<li>\(\phi_{j}(x)\): arbitrary function of x</li>
<li>\(w_{0},w_{1},...,w_{m}\): parameters (<strong>weights</strong>)</li>
<li>\(\phi_{1}(x),\phi_{2}(x),...,\phi_{m}(x)\): feature or basis functions</li>
</ul>

<h3>Common Examples of Basis Functions:</h3>

<ul>
<li><strong>High order polynomial with 1-Dimensional input \(x = (x_{1})\)</strong>:
<ul>
<li>\(\phi_{1}(x) = x , \phi_{2}(x) = x^{2} ,\phi_{3}(x) = x^{3}\)</li>
</ul></li>
<li><strong>Multidimensional quadratic \(\vec{x} = (x_{1} , x_{2})\)</strong> :
<ul>
<li>\(\phi_{1}(x) = x_{1} , \phi_{2}(x) = x_{1}^{2} ,\phi_{3}(x) = x_{2},\phi_{4}(x) = x_{2}^{2}, \phi_{5}(x) = x_{1}x_{2}\)</li>
</ul></li>
<li><strong>Other</strong>:
<ul>
<li>\(\phi_{1}(x) = sin(x), \phi_{2}(x) = cos(x)\)</li>
</ul></li>
<li><strong>Polynomials</strong>: \(\phi_{j(x)}= x^{j}\)</li>
<li><strong>Gaussians</strong>: \(\phi_{j}(x) = exp{\frac{x-\mu_{j}^{2}}{2s^{2}}}\)</li>
</ul>

<h2>Fitting Additive Linear Models:</h2>

<ul>
<li>To fit the model, we define an <strong>error function</strong>:
$$
J_{n}(w)= Error(\vec{w}) = \frac{1}{n} \sum\limits_{i=1}^{n} (y_{i}-f(x_{i}))^{2}
$$</li>
<li>We assume:
$$
\varphi(x_{i}) = (1,\phi_{1}(x_{i} ),\phi_{2}(x_{i}),...,\phi_{m}(x_{i}))
$$</li>
<li>So the <strong>Gradient Descent Update</strong>: 
$$
\nabla_{w}(J_{n}(w)) = \frac{-2}{n} \sum\limits_{i=1}^{n} (y_{i}- f(x_{i} )) \varphi(x_{i}) = \vec{0}
$$</li>
<li><p>This leads to a system of \(m\) linear equations given by:
$$
\sum\limits_{i=1}^{n} y_{i}\phi_{j}(x_{i})
$$</p>

<h2>Example of Learning with basis functions: Polynomial basis:</h2></li>
<li><p>Example: Regression with polynomials of degree \(m\)
$$
f(x,w) = w_{0}+ \sum\limits_{i=1}^{m} w_{i} \phi_{i}(x) = w_{0}+\sum\limits_{i=1}^{m} w_{i}x^{i}
$$</p></li>
<li><p><strong>Online Gradient update</strong> for \(<x,y>\) pair:
$$
\begin{align}
w_{0}= w_{0} + \alpha(y-f(x,w)) \
......................... \
w_{j}= w_{j} + \alpha(y-f(x,w)) x_{j}
\end{align}
$$</p>

<h2>Statistical Perspective of Linear Models:</h2></li>
<li><p><strong>Generative Model</strong>: \(y = f(x,w) + \epsilon\) </p>

<ul>
<li>\(f(x,w)\): <strong>deterministic Function</strong></li>
<li>\(\epsilon\): random noise \(\mathcal{N}(0,\sigma^{2})\)</li>
</ul></li>
<li>Assume that \(f(x,w) = w^{T}x\) is a linear model 
<ul>
<li>Then: \(f(x,w) = E(y|x)\) models the mean of outputs of \(y\) for \(x\)</li>
</ul></li>
<li>That means the model defines the conditional density of \(y\) given by \(x,w,\sigma\):
$$
p(y|x,w,\sigma) = \frac{1}{\sigma\sqrt{2\pi}} exp[\frac{-1}{2\sigma^{2}} (y-f(x,w))^{2}]
$$</li>
<li><strong>Likelihood of predictions</strong>: probability of observing outputs \(y\) in \(D\) given \(w,\sigma\)
$$
L(D,w,\sigma) = \Pi_{i=1}^{n} p(y_{i|}x_{i},w,\sigma)
$$</li>
<li>Can use the log likelihood trick to solve for maximizing the likelihood as:
$$
l(D,w,\sigma) = \frac{-1}{2\sigma^{2}} \sum\limits_{i=1}^{n} (y_{i} - f(x_{i}, w))^{2} + C(\sigma)
$$

</body>
</html>
